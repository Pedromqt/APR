{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M6ZkvKlybgAn"
   },
   "source": [
    "<div style=\"width: 100%; overflow: hidden;\">\n",
    "    <a href=\"http://www.uc.pt/fctuc/dei/\">\n",
    "    <div style=\"display: block;margin-left: auto;margin-right: auto; width: 50%;\"><img src=\"https://eden.dei.uc.pt/~naml/images_ecos/dei25.png\"  /></div>\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score,  mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHJ6PlU9bgAo"
   },
   "source": [
    "<h2><font color='#3498db'>1. Introduction</font></h2>\n",
    "In the previous exercise sheet we saw that of the main limitations of perceptrons arises from this theorem in that they are unable to learn complex patterns that are not linearly separable. To address this, multilayer perceptrons (MLPs) were introduced, consisting of multiple layers of perceptrons sequentially aranged, so that the output of the ith layer is the input of layer i+1. \n",
    "\n",
    "The objective of this exercise is for you to implement and test a Artificial Neural Network (ANN) with one single layer. An example of such network can be seen in the figure bellow.\n",
    "\n",
    "<center> \n",
    "<img width=\"250\" height=\"250\" src=\"ann.png\"/>\n",
    "<br>\n",
    "Artificial Neural Network with one hidden layer </center>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "We will assume that there are $n$ input nodes, plus the bias, $h$ hidden nodes, also plus a bias, and $o$ output nodes. We will assume a fully connected arquitecture, i.e., all neurons connect to all neurons in the next layer. As such there will be $(n+1) * h$ weights between the input and the hidden layer and  $(h+1) * o$ weights between the hidden layer and the output. We will assume that the bias values are equal to 1.\n",
    "\n",
    "The algorithm described could have any number of hidden layers, in which case there might be different values of $h$, and we would require an extra set of weights for the hidden layers.\n",
    "\n",
    "Assume the following:\n",
    "- $x$ is an array with the inputs\n",
    "- $w_1$ is the array with the weights from the input layer to the hidden layer\n",
    "- $w_2$ is the array with the weights from the hidden layer to the output layer\n",
    "\n",
    "\n",
    "### 1.1 The Algorithm\n",
    "\n",
    "##### Initialisation \n",
    "- Initialise two arrays of weights $w_1$ and $w_2$ to small random numbers. They can be positive or negative\n",
    "\n",
    "##### Training\n",
    "- repeat\n",
    "    - **Forwards pass**\n",
    "        - compute the outputs of the hidden layer $$z_1 = \\phi(x \\cdot w_1)$$  \n",
    "        - compute the outputs of the output layer $$\\hat{y}=f(z_1 \\cdot w_2)$$\n",
    "    - **Backwards pass**\n",
    "        - compute the error at the output layer: $$\\delta_o=(y - \\hat{y})f'(\\hat{y})$$\n",
    "        - compute the error at the hidden layer: $$\\delta_h=f'(z_1) * (\\delta_o \\cdot w_2^T) $$\n",
    "        - update the weights of the output layer: $$\\Delta w_2 = (z_1^T \\cdot \\delta_o) \\\\ w_2=w_2 - \\eta*\\Delta w_2$$\n",
    "        - update the weights of the hidden layer: $$\\Delta w_1 = (x^T \\cdot \\delta_h) \\\\ w_1=w_1 - \\eta*\\Delta w_1$$\n",
    "           \n",
    "\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CcAhYykobgAs"
   },
   "source": [
    "<h2><font color='#3498db'>2. Multi-Layered Perceptron Code</font></h2>\n",
    "Now it is time for you to code a Multi-Layered Perceptron Code. Using the information above and the input data, develop a class named `MultiLayerPerceptron` that learns based on a set of labeled examples.\n",
    "\n",
    "Since we are dealing mostly with matrix operations, I recommend that you use the `Numpy`numerical library for Python. this library provides us with several methods for matrix the manipulations, such as the Dot product: https://numpy.org/doc/stable/reference/generated/numpy.dot.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow your can find the implementation of two activation functions (and their derivatives) commonly used in ANNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, der=False):\n",
    "    if (der==True) : #derivative \n",
    "        f = 1/(1+ np.exp(-x))*(1-1/(1+ np.exp(- x)))\n",
    "    else : # sigmoid\n",
    "        f = 1/(1+ np.exp(-x))\n",
    "    return f\n",
    "\n",
    "# We may employ the Rectifier Linear Unit (ReLU)\n",
    "def relu(x, der=False):\n",
    "    if (der == True): # the derivative of the ReLU is the Heaviside Theta\n",
    "        f = np.heaviside(x, 1)\n",
    "    else :\n",
    "        f = np.maximum(x, 0)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color='#6A30BB'>Exercise #2.1 </font></h3>\n",
    "Code the MultiLayerPerceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T09:43:14.541171Z",
     "start_time": "2019-10-04T09:43:14.531279Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dSMOHMFubgAt"
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, number_of_inputs, n_targets, n_hidden, number_of_iterations=10000, learning_rate=0.01, output_type='relu'):\n",
    "        # Netowrk Parameters\n",
    "        self.number_of_iterations = number_of_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.number_of_inputs = number_of_inputs\n",
    "        self.number_of_outputs = n_targets\n",
    "        self.n_hidden = n_hidden\n",
    "        self.output_type = output_type\n",
    "\n",
    "        #Weight Initialisation\n",
    "        self.weights1 = (np.random.rand(self.number_of_inputs+1,self.n_hidden)-0.5)*2/np.sqrt(self.number_of_inputs)\n",
    "        self.weights2 = (np.random.rand(self.n_hidden+1,self.number_of_outputs)-0.5)*2/np.sqrt(self.n_hidden)\n",
    "    \n",
    "    \n",
    "    def add_bias(self, inputs):\n",
    "        \"\"\"\n",
    "            The bias is a parameter that is added to the weighted sum of the inputs before applying the activation function. \n",
    "            Its purpose is to provide an additional degree of freedom to the model, allowing it to shift the activation function \n",
    "            to the left or right. \n",
    "            To simplify the implementation of the perceptron (or the neural networks) a bias with the value of 1 is concatenated to every input vector.\n",
    "            This way the bias term can be included in the dot product operation between the weights and the inputs. This makes it possible to handle the bias \n",
    "            as part of the weight matrix. Without this, the bias would need to be added explicitly as a separate step after computing the weighted sum of the inputs.\n",
    "            For example, consider the inputs for the logical OR:\n",
    "            [   [1. 1.]\n",
    "                [1. 0.]\n",
    "                [0. 1.]\n",
    "                [0. 0.]\n",
    "            ]\n",
    "        The bias will be added as a third input, and the final set of inputs will be:\n",
    "            [   [1. 1. 1.]\n",
    "                [1. 0. 1.]\n",
    "                [0. 1. 1.]\n",
    "                [0. 0. 1.]\n",
    "            ]\n",
    "        \"\"\"\n",
    "        return np.concatenate((inputs,np.ones((inputs.shape[0],1))),axis=1)\n",
    "    \n",
    "\n",
    "    def predict(self, inputs):\n",
    "        inputs = self.add_bias(inputs)\n",
    "        return self.forward_propagation(inputs)\n",
    "    \n",
    "    def forward_propagation(self, inputs):\n",
    "        ## YOUR CODE HERE ##\n",
    "        a1 = np.dot(inputs,self.weights1)\n",
    "        a1 = relu(a1)\n",
    "        a1 = self.add_bias(a1)\n",
    "        \n",
    "        a2 = np.dot(a1,self.weights2)\n",
    "        a2 = sigmoid(a2)\n",
    "        return a1,a2\n",
    "        \n",
    "\n",
    "    def train(self, training_inputs, targets):\n",
    "        #Add the Bias to the Input Matrix as as a fixed input.\n",
    "        training_inputs = training_inputs = self.add_bias(training_inputs)\n",
    "        updatew1 = np.zeros((np.shape(self.weights1)))\n",
    "        updatew2 = np.zeros((np.shape(self.weights2)))\n",
    "        \n",
    "        for n in range(self.number_of_iterations):\n",
    "            ## YOUR CODE HERE ##\n",
    "            #last layer slide 18\n",
    "            a1,a2 = self.forward_propagation(training_inputs)\n",
    "        \n",
    "            E2 = targets - a2\n",
    "            s2 = E2 * sigmoid(a2, True)\n",
    "            self.weights2 += self.learning_rate * np.dot(a1.T, s2)\n",
    "\n",
    "            s1 = np.dot(s2, self.weights2.T) * relu(a1, True) \n",
    "            s1 = s1[:, :-1] # tirar o bias do camada hidden\n",
    "            self.weights1 += self.learning_rate * np.dot(training_inputs.T, s1)\n",
    "            \n",
    "    def accuracy(self,a2,target):\n",
    "        predict = (a2 > 0.5).astype(int)\n",
    "        if (np.mean(predict == target)==1):\n",
    "            print(\"gg\")\n",
    "        else:\n",
    "            print(\"learn more pleasee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color='#6A30BB'>Exercise #2.2 </font></h3>\n",
    "Now let's go back to the problem where the Perceptron failed. Train the MLP to learn the XOR Logical Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn more pleasee\n"
     ]
    }
   ],
   "source": [
    "# Logical XOR \n",
    "training_inputs = np.array([[1, 1], [1, 0], [0, 1], [0, 0]])\n",
    "labels = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "## YOUR CODE HERE ##\n",
    "mlp = MultiLayerPerceptron(2,1,4)\n",
    "mlp.train(training_inputs,labels)\n",
    "a1,a2 = mlp.predict(training_inputs)\n",
    "mlp.accuracy(a2,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color='#6A30BB'>Exercise #2.3 </font></h3>\n",
    "Evaluate the capacity of the MLP to learn the XOR function. How close did it get to the perfect answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logical XOR \n",
    "\n",
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color='#6A30BB'>Exercise #3 </font></h3>\n",
    "Lets evaluate the performance of your model in a dataset beyond the binary ones. In this case we are going to classfy the iris dataset.\n",
    "\n",
    "The Iris dataset was used in R.A. Fisher's classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems, and can also be found on the UCI Machine Learning Repository.\n",
    "\n",
    "It includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\n",
    "\n",
    "The columns in this dataset are:\n",
    "\n",
    "- SepalLength (cm)\n",
    "- SepalWidth (cm)\n",
    "- PetalLength (cm)\n",
    "- PetalWidth (cm)\n",
    "- Species\n",
    "\n",
    "Let's start by loading and preparing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=10)\n",
    "three = np.zeros((y_train.shape[0], 3))\n",
    "three[np.where(y_train == 0),0] = 1\n",
    "three[np.where(y_train == 1),1] = 1\n",
    "three[np.where(y_train == 2),2] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.87110766, -0.39891058,  0.46061935,  0.10579946],\n",
       "       [ 0.39378839, -0.39891058,  0.29192864,  0.10579946],\n",
       "       [ 1.58708656, -0.16765807,  1.13538218,  0.49445053],\n",
       "       [-0.08353087, -0.63016309,  0.74177053,  1.53085339],\n",
       "       [ 0.51311821, -1.32392062,  0.68554029,  0.8831016 ],\n",
       "       [-1.51548867,  0.29484695, -1.3387482 , -1.31925447],\n",
       "       [ 0.99043748,  0.52609946,  1.07915194,  1.14220232],\n",
       "       [ 1.22909711,  0.29484695,  1.07915194,  1.40130303],\n",
       "       [ 2.18373564, -1.09266811,  1.75391477,  1.40130303],\n",
       "       [ 1.22909711,  0.06359444,  0.74177053,  1.40130303],\n",
       "       [-1.0381694 ,  0.75735197, -1.22628772, -1.06015376],\n",
       "       [-1.0381694 ,  0.98860448, -1.22628772, -0.80105304],\n",
       "       [-0.79950977, -0.8614156 ,  0.0670077 ,  0.23534982],\n",
       "       [-1.63481849, -1.78642564, -1.39497843, -1.18970411],\n",
       "       [ 0.51311821,  0.52609946,  0.51684958,  0.49445053],\n",
       "       [-0.79950977,  2.37611954, -1.28251796, -1.44880483],\n",
       "       [ 1.22909711,  0.06359444,  0.62931005,  0.36490017],\n",
       "       [ 0.03579894,  0.29484695,  0.57307982,  0.75355124],\n",
       "       [-0.3221905 , -0.63016309,  0.62931005,  1.01265196],\n",
       "       [ 0.99043748,  0.52609946,  1.07915194,  1.66040374],\n",
       "       [ 0.39378839, -2.01767815,  0.40438911,  0.36490017],\n",
       "       [ 1.58708656,  1.21985699,  1.30407288,  1.66040374],\n",
       "       [-0.44152032, -1.55517313, -0.04545277, -0.28285161],\n",
       "       [ 0.15512876, -0.39891058,  0.40438911,  0.36490017],\n",
       "       [ 0.63244803,  0.29484695,  0.40438911,  0.36490017],\n",
       "       [-0.08353087,  2.14486703, -1.45120867, -1.31925447],\n",
       "       [-0.68017995,  1.4511095 , -1.28251796, -1.31925447],\n",
       "       [ 0.27445858, -0.39891058,  0.51684958,  0.23534982],\n",
       "       [-0.56085014,  0.75735197, -1.17005749, -1.31925447],\n",
       "       [ 0.63244803,  0.06359444,  0.96669147,  0.75355124],\n",
       "       [-1.39615885,  0.29484695, -1.22628772, -1.31925447],\n",
       "       [-1.51548867,  0.06359444, -1.28251796, -1.31925447],\n",
       "       [-1.15749922, -1.32392062,  0.40438911,  0.62400089],\n",
       "       [-0.91883959, -1.32392062, -0.43906442, -0.15330126],\n",
       "       [ 0.99043748,  0.06359444,  1.0229217 ,  1.53085339],\n",
       "       [-1.15749922,  1.21985699, -1.3387482 , -1.44880483],\n",
       "       [ 1.22909711,  0.06359444,  0.91046123,  1.14220232],\n",
       "       [-1.27682904, -0.16765807, -1.3387482 , -1.18970411],\n",
       "       [-0.08353087, -0.8614156 ,  0.0670077 , -0.0237509 ],\n",
       "       [ 0.15512876, -0.8614156 ,  0.74177053,  0.49445053],\n",
       "       [-1.7541483 ,  0.29484695, -1.39497843, -1.31925447],\n",
       "       [ 1.10976729,  0.29484695,  1.19161241,  1.40130303],\n",
       "       [ 2.42239527,  1.68236201,  1.47276359,  1.01265196],\n",
       "       [ 0.75177784, -0.16765807,  0.96669147,  0.75355124],\n",
       "       [ 2.06440582, -0.16765807,  1.58522406,  1.14220232],\n",
       "       [ 0.51311821, -0.63016309,  0.74177053,  0.36490017],\n",
       "       [-1.51548867,  0.75735197, -1.3387482 , -1.18970411],\n",
       "       [-0.20286069, -1.09266811, -0.15791325, -0.28285161],\n",
       "       [ 0.99043748, -1.32392062,  1.13538218,  0.75355124],\n",
       "       [-0.20286069, -0.63016309,  0.17946817,  0.10579946],\n",
       "       [-1.15749922,  0.06359444, -1.28251796, -1.31925447],\n",
       "       [ 0.63244803, -0.63016309,  1.0229217 ,  1.14220232],\n",
       "       [ 1.34842693,  0.29484695,  0.51684958,  0.23534982],\n",
       "       [-0.20286069, -0.39891058,  0.2356984 ,  0.10579946],\n",
       "       [-1.0381694 ,  1.21985699, -1.3387482 , -1.31925447],\n",
       "       [-0.56085014,  1.91361452, -1.17005749, -1.06015376],\n",
       "       [-0.91883959,  1.68236201, -1.05759702, -1.06015376],\n",
       "       [-0.56085014, -0.16765807,  0.40438911,  0.36490017],\n",
       "       [ 1.82574619, -0.63016309,  1.30407288,  0.8831016 ],\n",
       "       [ 0.51311821, -0.39891058,  1.0229217 ,  0.75355124],\n",
       "       [ 0.87110766, -0.16765807,  0.34815888,  0.23534982],\n",
       "       [-1.0381694 ,  0.75735197, -1.28251796, -1.31925447],\n",
       "       [-0.91883959,  1.68236201, -1.22628772, -1.31925447],\n",
       "       [-0.91883959,  0.98860448, -1.3387482 , -1.18970411],\n",
       "       [ 0.75177784, -0.16765807,  1.13538218,  1.27175267],\n",
       "       [-0.08353087, -0.8614156 ,  0.74177053,  0.8831016 ],\n",
       "       [ 0.27445858, -0.63016309,  0.12323793,  0.10579946],\n",
       "       [-0.44152032, -1.55517313,  0.01077746, -0.15330126],\n",
       "       [ 0.75177784,  0.29484695,  0.74177053,  1.01265196],\n",
       "       [ 0.75177784, -0.16765807,  0.79800076,  1.01265196],\n",
       "       [ 0.03579894, -0.16765807,  0.74177053,  0.75355124],\n",
       "       [ 0.27445858, -1.09266811,  1.0229217 ,  0.23534982],\n",
       "       [ 0.63244803, -0.39891058,  0.29192864,  0.10579946],\n",
       "       [-0.79950977,  0.75735197, -1.3387482 , -1.31925447],\n",
       "       [-1.27682904,  0.75735197, -1.22628772, -1.31925447],\n",
       "       [-0.3221905 , -0.8614156 ,  0.2356984 ,  0.10579946],\n",
       "       [-0.91883959,  0.52609946, -1.17005749, -0.9306034 ],\n",
       "       [-1.51548867,  1.21985699, -1.56366914, -1.31925447],\n",
       "       [ 0.27445858, -0.16765807,  0.62931005,  0.75355124],\n",
       "       [-1.0381694 , -1.78642564, -0.27037372, -0.28285161],\n",
       "       [-0.20286069,  1.68236201, -1.17005749, -1.18970411],\n",
       "       [-0.79950977,  0.98860448, -1.28251796, -1.31925447],\n",
       "       [-0.44152032,  0.98860448, -1.39497843, -1.31925447],\n",
       "       [-1.15749922, -1.55517313, -0.27037372, -0.28285161],\n",
       "       [-0.56085014,  0.75735197, -1.28251796, -1.06015376],\n",
       "       [ 0.99043748,  0.06359444,  0.34815888,  0.23534982],\n",
       "       [-1.27682904, -0.16765807, -1.3387482 , -1.44880483],\n",
       "       [-0.44152032, -1.32392062,  0.12323793,  0.10579946],\n",
       "       [ 0.15512876, -2.01767815,  0.68554029,  0.36490017],\n",
       "       [-1.27682904,  0.06359444, -1.22628772, -1.31925447],\n",
       "       [ 0.99043748,  0.06359444,  0.51684958,  0.36490017],\n",
       "       [-0.08353087, -1.09266811,  0.12323793, -0.0237509 ],\n",
       "       [ 0.39378839,  0.75735197,  0.91046123,  1.40130303],\n",
       "       [-1.0381694 , -0.16765807, -1.22628772, -1.31925447],\n",
       "       [ 0.15512876, -0.16765807,  0.57307982,  0.75355124],\n",
       "       [-1.87347812, -0.16765807, -1.5074389 , -1.44880483],\n",
       "       [-0.3221905 , -1.32392062,  0.0670077 , -0.15330126],\n",
       "       [ 0.99043748, -0.16765807,  0.68554029,  0.62400089],\n",
       "       [ 2.18373564, -0.16765807,  1.30407288,  1.40130303],\n",
       "       [ 0.51311821,  0.75735197,  1.0229217 ,  1.53085339],\n",
       "       [-0.44152032,  2.60737205, -1.3387482 , -1.31925447],\n",
       "       [ 0.15512876, -2.01767815,  0.12323793, -0.28285161],\n",
       "       [ 2.18373564, -0.63016309,  1.6414543 ,  1.01265196],\n",
       "       [ 1.70641637, -0.39891058,  1.41653335,  0.75355124],\n",
       "       [-0.3221905 , -0.16765807,  0.17946817,  0.10579946],\n",
       "       [ 0.75177784, -0.63016309,  0.46061935,  0.36490017],\n",
       "       [ 0.51311821,  0.52609946,  1.24784265,  1.66040374],\n",
       "       [-0.56085014,  1.91361452, -1.39497843, -1.06015376],\n",
       "       [ 0.63244803,  0.29484695,  0.854231  ,  1.40130303],\n",
       "       [-1.0381694 ,  0.98860448, -1.39497843, -1.18970411],\n",
       "       [-0.91883959,  0.98860448, -1.3387482 , -1.31925447],\n",
       "       [ 0.27445858, -0.63016309,  0.51684958, -0.0237509 ],\n",
       "       [-1.7541483 , -0.39891058, -1.3387482 , -1.31925447],\n",
       "       [ 0.99043748, -0.16765807,  0.79800076,  1.40130303],\n",
       "       [ 0.51311821, -0.8614156 ,  0.62931005,  0.75355124],\n",
       "       [-0.20286069, -1.32392062,  0.68554029,  1.01265196],\n",
       "       [-0.3221905 , -0.39891058, -0.10168301,  0.10579946],\n",
       "       [-0.20286069,  3.06987706, -1.28251796, -1.06015376],\n",
       "       [ 1.58708656,  0.29484695,  1.24784265,  0.75355124],\n",
       "       [-1.15749922,  0.06359444, -1.28251796, -1.44880483]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create an instance of our MLP and train it for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can compute the confusion matrix and accuracy. Let's start with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can do the same with the test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color='#6A30BB'>Exercise #4 </font></h3>\n",
    "\n",
    "Now that we saw how to implement a MLP from scratch, let's explore how ***Pytorch*** can make our lives easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    " \n",
    "X_train = torch.Tensor(X_train)\n",
    "y_train = torch.Tensor(three)\n",
    "\n",
    "## Define the arquitecture in Torch by using nn.Sequential in this case\n",
    "#Docs: https://pytorch.org/docs/stable/nn.html\n",
    "# We can add more or less layers, here we are replicating the above arquitecture but with RELU activations. \n",
    "# Change to the other options and see the effect\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(4,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,3)\n",
    ")\n",
    "\n",
    "### Optimizers will be object of study in another time. Nevertheless there are different options available that can be used to \n",
    "# optimize the weights of the models\n",
    "# https://pytorch.org/docs/stable/optim.html\n",
    "# here we are going to use Stochastic Gradient Descent\n",
    "# We will have the chance to check and experiment with others, nevertheless here are the implemented ones in torch:\n",
    "# https://pytorch.org/docs/stable/optim.html#algorithms\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# We can also explore different loss functions\n",
    "# try exchanging into the different options bellow\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "#loss_fn = torch.nn.L1Loss()\n",
    "#loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Number of epochs is the number of passages on the full dataset. \n",
    "num_epochs = 10000\n",
    "\n",
    "# optional to check and use gpu support\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# depending on the device we should specify where is the model and data are suppose to run\n",
    "model.to(device)\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    if n%1000 == 0:\n",
    "        print(n, loss)\n",
    "    \n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    # backward step or backpropagation of the error\n",
    "    loss.backward()\n",
    "    # perform optimizer steps\n",
    "    optimizer.step()\n",
    "print(num_epochs, loss)\n",
    "\n",
    "# after training, prepare test tensors\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_test = torch.Tensor(y_test)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "# compute predictions into a vector\n",
    "test_predictions = torch.argmax(model(X_test), dim=1)\n",
    "\n",
    "test_predictions = test_predictions.cpu()\n",
    "y_test = y_test.cpu()\n",
    "\n",
    "# create confusion matrix and check accuracy metric \n",
    "# During Theoretical classes we have seen more metrics that could be used.\n",
    "# Explore based on what is implemented or compute them by yourselves.\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "# E.g. calculate F1, sensitivity and recall\n",
    "print(\"Confusion Matrix\\n\", confusion_matrix(y_test, test_predictions))\n",
    "print(\"Accuracy: %.2f %%\" % (accuracy_score(y_test, test_predictions) * 100))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TDX-UC-DSA-X-Practice-X-Complete.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
